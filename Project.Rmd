---
title: "MLproject"

---
#Load in files
```{r}
library(caret);library(rpart);library(rattle);library(rpart)

TrainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TrainData <- read.csv(TrainUrl)

TestUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
Testing <- read.csv(TestUrl)
```
#Clean data (Add correct Class to Features, Remove NA and Features that arent important)

```{r}
Structure <- str(TrainData)##look at to exame data
##Remove Index (X), Nmaes and Time stamps
TrainData <- TrainData [ ,-c(1:5)]
## Find features that were incorrecty Classified
ColClass <- sapply(TrainData[,-155],function(x)class(x))
ColFactors <- as.vector(which(ColClass=="factor"))

DataSum <- summary(TrainData[,ColFactors])
TrainData[,ColFactors] <- lapply(TrainData[,ColFactors],function(x)as.numeric(x))

##look for features that have a lot of NA values
NaFeatures <- as.vector(apply(TrainData,2,function(x)sum(is.na(x))))

## remove features that contin mainly na and have very little variance
plot(NaFeatures)
abline(a=mean(NaFeatures),b=0,col="red")
TrainData <- TrainData[,-which(NaFeatures > mean(NaFeatures))]

### Find feature that are unlikely to be important ie little variance 
NotImprtant <- nearZeroVar(TrainData,saveMetrics = TRUE)
NotImpIndex <- which(NotImprtant$nzv=="TRUE")

NotImPlIST <- list()
for (i in 1: length(NotImpIndex)){
     tabNotImp <- table(TrainData[,NotImpIndex[i]])
      NotImPlIST[[i]] <- tabNotImp[tabNotImp>2]
}
Training <- TrainData[,-NotImpIndex]
```

#Create a training, Test and Validation data-set

```{r}
index <- createDataPartition(Training$classe,p=0.6,list=FALSE)
TrainSet <- Training[index,]
TestSet <-  Training[-index,]
index2 <- createDataPartition(TestSet$classe,p=0.6,list=FALSE)
validation <- TestSet[-index2,]
TestSet <- TestSet[index2,]
```

# Model 1-- A classification tree
```{r}
## fit a classification tree
Modfit1 <- train(classe~.,method = "rpart",data = TrainSet)
preTest1 <- predict(Modfit1,newdata = TestSet)
fit1Results <- confusionMatrix(TestSet$classe,preTest1)
fit1Results[c(2:3)]##Accuracy quite poor Bad!
plot(preTest1)
```
This Model is not very good large out of sample error!

try Cross validation to improve accuracy
```{r}
## Use Cross Valadation instead of bootstrap?
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
Modfit2 <- train(classe~.,method = "rpart",data = TrainSet, trControl = fitControl)
preTest2 <- predict(Modfit2,newdata = TestSet)
fit2Results <- confusionMatrix(TestSet$classe,preTest2)
fit2Results[c(2:3)]##Accuracy still very poor no difference!
plot(preTest2)
```
Results similar !! will stick with CV but will use dfferent models to try to improve.

# Model 3 -Fitting  a logistic / Multinomial Model

```{r}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
Modfit3 <- train(classe~.,method = "multinom",data = TrainSet, trControl = fitControl)
preTest3 <- predict(Modfit3,newdata = TestSet)
fit3Results <- confusionMatrix(TestSet$classe,preTest3)
plot(preTest3)
fit3Results[c(2:3)]##Accuracy still okay but getting better!
```
Out of sample error is a little better, still not great, will try and fit an alternative model


# Model 4 -Linear Discriminant Analysis (LDA)
```{r}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
Modfit4 <- train(classe~.,method = "lda",data = TrainSet, trControl = fitControl)
preTest4 <- predict(Modfit4,newdata = TestSet)
fit4Results <- confusionMatrix(TestSet$classe,preTest4)
plot(preTest4)
fit4Results[c(2:3)]
```
Out of sample error still poor but getting better!

# Model 5 - Classification tree (ct)
```{r}
##fit a logitModel
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
Modfit5 <- train(classe~.,method = "ctree",data = TrainSet, trControl = fitControl)
preTest5 <- predict(Modfit5,newdata = TestSet)
fit5Results <- confusionMatrix(TestSet$classe,preTest5)
plot(preTest5)
fit5Results[c(2:3)]##Accuracy good, pure luck on model selection!
```
Out of sample error good!

#Model 6 (stacking)
Investigate if model selection/ ensembling is wort exploring
```{r}
library("caretEnsemble")
model_list <- caretList(classe~., data=TrainSet,
methodList=c('ctree', 'rpart',"lda","multinom"),trControl = fitControl)
modelCor(resamples(model_list))
```
some models are highly correlated, lets use all models anyway!

```{r}
predDF <- data.frame(preTest2,preTest3,preTest4,preTest5,classe=TestSet$classe)
comModFit <- train(classe~.,method="rf",data=predDF)
comPred <- predict(comModFit,predDF)
fit6Results <- confusionMatrix(TestSet$classe,comPred)
fit6Results[c(2:3)]
```
Accuracy best so far!

#Model 7 - Now try randomforest; (tried not to)!
```{r}
fitControl <- trainControl(method="cv", number=3, verboseIter=F)
Modfit7 <- train(classe~.,method = "rf",data = TrainSet, trControl = fitControl)
preTest7 <- predict(Modfit7,newdata = TestSet)
fit7Results <- confusionMatrix(TestSet$classe,preTest7)
fit7Results[c(2:3)]##Accuracy the best!
```
Out of sample error very good!!
Use Model on Validation dataset
```{r}
preTestVal <- predict(Modfit7,newdata = validation)
fitValResults <- confusionMatrix(validation$classe,preTest7)
ValAccuracy <- fitValResults[c(2:3)]##Accuracy the best!
OutSample <- (1-ValAccuracy$overall[1])*100
```
Out of sample Error is very Low

# Model Selection 
Will Use Random Forest as the Prediction model

```{r}
Testing <- Testing[,colnames(Testing)%in%colnames(Training)]

predFinal <- predict(Modfit7, Testing, type = "raw")
```

Ready to try on test set!!
```{r}
pml_write_files = function(x){
      n = length(x)
      for(i in 1:n){
            filename = paste0("problem_id_",i,".txt")
            write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
      }
}
pml_write_files(predFinal)

```
